

Deploying our Vegetable ohenotyping pipeline in Loc360 Enterprise Solution:


1. **sagemaker_preprocess(img_path, metadata)** - "metadata[“properties”][“dl_model_name”]" contains the first model name to be executed.  Do preprocessing from model #1, return string payload to send inference request to model #1.  Basically, does preprocessing for mode #1

2. **sagemaker_preprocess(img_path, metadata)** – "metadata[“properties”][“sagemaker_response”]" contains raw inference response string from model #1 so you we can do model #1  postprocessing here to prepare for model #2 call.  "metadata[“properties”][“dl_model_name”]" now contains the second model name to be executed.  Do preprocessing for model #2 (you should be able to since you have access to the response from model #1).  Return string payload to send for inference request to model #2.  Basically, does postprocessing for model #1, preprocessing for model #2 (call your step 2 and 3 functions)

3. **process(img_path, metadata)** - "metadata[“properties”][“sagemaker_response”]" contains the raw inference response string from model #2.  Do your work, return a dictionary of metrics that you want saved to imagine.  Basically, does post processing for model #2 and returns a dictionary metrics values that will be saved to the metrics imagine collection. (redirect call to step 4 function from your example)
What’s new in our workflow is that we’re calling multiple models for one single python pre/postprocessing module since the first model is essentially doing preprocessing work for the second model and not producing any metrics itself.  This feature is currently undergoing testing right now, so we’ll get to play with it soon. If you need to make more than one call of a given dl model after preprocessing, we support that, just return a stringified list and imagery analytics can be configured to treat that list as a list of individual calls.
 
The larger point being: when these functions are invoked you may redirect the call to your own internal individual functions.  If you do this obviously you can use whatever interface is most convenient for you (you could read in the data from the meta passed by imagery analytics, to get “points”, determine value of the segmentation bool flag based on the dl model name, etc and then invoke the appropriate function in your module). What you do in your own internal preprocessing and processing functions doesn’t matter to imagery analytics – the functions invoked by the platform will always be consistently named and have consistent arguments.

The code conforming with the above instructions should be pushed to  [Loc360](https://github.platforms.engineering/location-360-imagery-analytics)
